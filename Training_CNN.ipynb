{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for digits and operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load of the datas numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAABXCAYAAAAj1Ay6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAbUUlEQVR4nO3dd5hcVf3H8feRDiEUgVBUEKTz0JsQmjTpQRAwNEFCR1FAQIoQQESkKCCIhJqAUv1RpRMhCAEJvYTQQ0CKEEKohvv7Y/PZM/fuzraZ2Xtm9/N6njzJzs7O3Dm5c/fO537P94QsyzAzMzMzS9HXyt4AMzMzM7NqfLJqZmZmZsnyyaqZmZmZJcsnq2ZmZmaWLJ+smpmZmVmyfLJqZmZmZsnyyaqZmZmZJSuZk9UQwrwhhBtCCFNDCK+FEIaWvU1lCiHcF0L4LITw8fQ/L5S9TWULIewSQnhu+j7yUghhvbK3qSzeP/JCCMuGEO4JIUwOIUwIIWxf9jaVJYQwSwhhxPTj6JQQwrgQwhZlb1dZPB5t+fjRvhDCktPHZWTZ21KmFM/HkjlZBc4DvgAGAbsC54cQli93k0p3cJZlA6b/WbrsjSlTCGFT4DRgL2BOYH3g5VI3qnzeP4AQwozA/wE3A/MC+wIjQwhLlbph5ZkReAPYAJgLOA64OoSwWInbVCaPR/t8/GjrPOCRsjciAcmdjyVxshpCmAPYATguy7KPsyx7ALgR2L3cLbOEnAgMz7LsoSzLvsqy7M0sy94se6MsCcsACwNnZVk2Lcuye4Ax9NPjR5ZlU7MsOyHLslenv1duBl4BVit728rg8bCuCCHsAnwI3F32tpQp1fOxJE5WgaWAaVmWja+47Qmgvyerp4YQ3gshjAkhbFj2xpQlhDADsDow//RLvBNDCOeGEGYre9tK5v2jRahy2wq9vSEpCiEMouUY+0zZ25ICj0crHz+mCyEMBIYDh5W9LQlI8nwslZPVAcDkwm2Tabnc218dCSwOLAJcCNwUQlii3E0qzSBgJmBHYD1gZWAV4NgyN6pk3j+i54F3gCNCCDOFEDaj5ZLv7OVuVvlCCDMBo4DLsix7vuztKZvHo5WPH3knASOyLHuj7A1JQJLnY6mcrH4MDCzcNhCYUsK2JCHLsoezLJuSZdnnWZZdRstlzS3L3q6SfDr973OyLHsry7L3gDPpv+Ph/aNClmVfAkOArYC3aUlHrgYmlrldZQshfA24gpbas4NL3pzSeTwiHz+iEMLKwCbAWWVvSyKSPB+bscwnrzAemDGEsGSWZS9Ov20lfJmmUkb7lzv7vCzLPgghTKRlDKx9/Xb/AMiy7Ela0lQAQggPApeVt0XlCiEEYAQtVyW2nH5C3295PDrVn48fGwKLAa+37CYMAGYIISyXZdmqJW5XWZI8H0siWc2ybCpwPTA8hDBHCGFdYDtaPgX3OyGEuUMIm4cQZg0hzBhC2JWW2e+3l71tJboEOCSEsEAIYR7gUFpmf/c73j/aCiGsOH08Zg8hHA4sBFxa8maV6XxgWWCbLMs+7ezO/YDHYzofP9q4EFiClvKylYELgFuAzcvcqLKkej6WSrIKcCBwMS21Z+8DB2RZ1l+T1ZmAk2mZ5TyNlpq8IVmW9edeeCcB89Hyqe8zWi7znlLqFpXH+0dbuwP70DI29wObZln2ebmbVI4QwqLAfsDnwNvT0yKA/bIsG1XahpXE49GGjx8Vsiz7BPhEX4cQPgY+y7Ls3fK2qnTJnY+FLPOVVTMzMzNLUxJlAGZmZmZm7fHJqpmZmZklyyerZmZmZpYsn6yamZmZWbJ8smpmZmZmyeqwdVUIoc+3CsiyrMuNkD0eeR6PPI9HWx6TPI9Hnscjz+OR5/HI68/j4WTVzMzMzJLlk1UzMzMzS5ZPVs3MzMwsWSktt2qWs+iiiwIwduxYACqWSWT11VcH4PXXX+/9DTMzM7Ne42TVzMzMzJLlZNWStc8++wDw9a9/HYAHHnigzM0xMzOzEjhZNTMzM7NkhSyr3rarUT29Vl11VQA23XRTAH7zm9+0uc/XvtZyHv3VV18BcPTRRwMwZswYAP7zn/8AMGHChJq2xT3O8lIYj/XXXx+A++67D4BTTjkFgOOOO64RT9ehFMYjJe6z2pb3kbwUxmOuueYCYK+99gJgpZVW0vO13mfOOecEYPvttwdg9OjRQDzO1OtKTgrjkRKPR16zjsd8880HwAEHHADAlClTADj77LNrelz3WTUzMzOzptMryeqKK64IwOabbw7AL37xCyCembenmKwWPfroowAMGzYMgKeffrpH25bCpxq91plmmqnqffbYYw8AvvWtb+Vu33///YGOxxJgm222AeDmm2/u8H4pjMdtt90GxNe6xhprAPDJJ5804uk6lMJ4dIUSJO1DF154YYf3Hzx4cO7rzTbbDIAZZ2wpYx80aBAAP/nJT3L3c7LaVgr7yDrrrAPAdtttB8DCCy8MwI477th6n1lmmaXdn7322msB2HXXXQH48ssva9qWMsdDaemZZ54JwN577118vtZ/v/vuuwDMPvvsub/1+nUF8Nlnn61pm1LYP4rOOOMMAH74wx+2+d5OO+0EwEMPPdSQ505xPMrULOOx0EILAbD77rsDsMsuuwDxqsVHH30EwEYbbQTA448/3qPncbJqZmZmZk2nod0AvvGNbwBw8cUXA/EMvB7UZ/PSSy8FYNtttwVg0qRJdXuO3nLIIYcAcNZZZ/X4MZSQKxX4/PPPc9//7LPPevzYveXnP/85EFO+xx57DCgnUU3ZzDPPDOTT09122w2AadOmAfC73/0OiPvFO++8A8ACCywAxJq+anT/1On1rLvuugBsuOGGrd+78847AbjnnnuAvrUf7bfffgDsvPPOQHz9SsaVIKqODGDcuHG5x1h66aWBmL7qeP29730PaI5jhuiKgo4hSlS1/+vKm65EAbzxxhtAHLurrroq91j6nVJrstpIa6+9NhCvVhZvVzraXoJapPF4880367mJSdM4/etf/wLy41jL7+O+aJFFFgHg1FNPbff7AwcOBOCmm24C4Jvf/GZdn9/JqpmZmZklq6HJ6r333gvAYost1rDnUFqrmeNLLbVUw56r3rRC04EHHljzY91///0AnHTSSQDcddddNT9mbxsyZAgQ05Drr7++zM1Jjvbtgw46CIh1zJWq1T93lqQ+88wzAIwfPx5Ib+yV+q2yyipA3FdUXzfbbLMB+ZpEjZM+6avW6uOPP+6FLW6MDTbYAIBzzjkHgBlmmKHD+99yyy2t/x46dGjue6pr/dvf/gbEulfN7m2mZOnggw8G4Ne//nXu9lGjRgGw5557Vv3ZanX8nc0DSIES086S02uuuQaI6enEiROBWK9c+b3+QKnf1VdfDcTXXjkeBjfccEPrvyuvWnVEv2s0V+nJJ5+sy7Y4WTUzMzOzZDU0WVXa15NkdcSIEUBM2TbZZBOg7Wx40SzQZqIUdMkll+zyz7z//vsAPPXUU0D8xHzllVcCMHny5HpuYq9ab731gPh/Xvmprj9bZpllALjuuuuAWEt42mmntd5HdYaaxa/aZc3QVC236jeff/55ICaqSlqqdd/obZqZrZ6XqkHUamaiGd0vvPACEGvPIO5P6oRx2WWXAbFzgsammej1Tp06FYh1YtonVJercVIHFohJqvYF/a0+10oYVcPaTMmqenYXnX766b28Jb3rsMMOy32tmkvVMis5tDx1Q1DCqnHrT+lye3QFS8cN1bVDPNZ0Zo455gDgjjvuAOCoo44C4vyinnKyamZmZmbJamiyquRHtWLVKB3UmTi0PQvfeOONgdhzU6lkM9KMXX0CqUardEGc+X3eeecBzTNbuyu0gkyxVlXpX3+lOky9P7Q/6FNvrf0wU6QZp5qZrTpKee+994BYZ3n++ecD7e8r6iv6gx/8AIh1jdX6jTYDzUzXDHZdaXr11VeBOA533303kO+V2dVOKcX0uhnod82CCy4IxNT8xRdf7PRnVQddVOyokjIlrKpdXWuttQAnq0XqFqFx0pWYrlxFKPamrXaVtxltvfXWAJxwwglA9fdEd8w///wAXHTRRQAMGDAAgHPPPbdHj+dk1czMzMyS1dBkde655+7w+1988QUQa1s7qmlQUqD7ihJWzYJWXUXK9WiaJadEsWjs2LFtvv/WW281fsNK8qtf/QqIM7n7e6Iq6hKx/PLLA3HNZXUFqKyvSnl/74o111wTiLPcV1tttdz3VZOt+squ7CNKxpTSaq131X1Ws8QSSwDw0ksvdWnby6CEtVoP0Ntvvx2AW2+9tdPHUi2r3n+VHRWaxejRo4G46lR3qCexvPzyy0C+JrxZ6CqMajB1zOjvtZjqp6qVzTQequ3tSLGutfiYjVrpq5G22morIF6pPvLII4HYw1t0JQtidycdi3UMnmeeeXK3F+co6Ypprf17nayamZmZWbIamqzeeOON7d6uRFWpqOrOukI/W1nPCTDvvPMCcMkllwCwww47dG9je4E+gejTb5Fm9B5++OFA305T2+MuAC10daDY1+7QQw8FYu2lZvJD/NT7pz/9CYAJEyY0ejPrQv1g1R9TieqHH34IwLBhw4C4GlUtPVKL6ZLWsFbXgO222w6Iq2IpWetpjVWZimlhe9RHVMcbvf90ZaevUz20Un1RstqMVysefvjh3NfVOjsoFWyv7vL1118HYhLWF1LZYiqq8ejKa9NxV/T7uxkTVfV2V7cl1ZUWXX755QCcfPLJrbd1dqVJaa16Oyth/d///gfU/nvdyaqZmZmZJashyeqPf/xjIPZKLNKn/t/+9reNePpkffvb3879XaQZveqVWFl/pU/56iSghFkpQF9QS62cPiHutttuQL4/HMRPdarlS5n+r5WSFNMP1VBVrrCjT//qt6rkJPX13bXd6nCg1/6jH/0IiIlqPWiW+8iRIwEYPHgwELsuFCntLSNZVbqu+uRHH3207s+h1E3vFaVMxx57bN2fK0Wq09N8B9lvv/3K2Jy6qOwzDHH2e/Hr7qzbriRRHQeaKWnVcbCz2f8aj8oUVT9T7zXuy6Arbt///veB6omqVnr7+9//DnTvSpZqWDW2SrPV4UXP/Y9//KNb2y5OVs3MzMwsWXVJVjW7XQmEVtyZddZZc/c79dRTga7VU/WU0hIlM5oJnIJ99tmnw+9r3NpLUTQrT8mqZjpfccUVABx//PFA+klaR1Qz1x3HHHMMEMdWKaQeSyt+aS15rXveDHWxqheqrBuCmBBW9oFUCqiexkqH/vCHPzR8O2uhFapE/YPrmagqIVGPYtW3d6bMbgBKmBuRqOoYqc4KotXNXnvttbo/ZyqUJkPbVa9GjRoF1D5ruUxKPZWGat/XLHgli6q3vPbaa1t/Vt/TGGk1Iz3GmDFjgNjjtxkS1u9+97u5rzUuSkvVr7l4v44U0+uU6XXqPa+a1aJaEtUinQeqq8pPf/pTINayzjDDDD16XCerZmZmZpasuiSrc845JxA/cVXzyiuvAPDpp5/W/JyqMyrWG+nTX0qJaj1U1idW0kxejf3PfvYzoDGJTKN1VrOqmheI67yr9uaf//wnEOtkisnpI488AsSers2QrFajtLjSEUccAcQURP0VlVSm+n7QKm5Kwh988MGaHk8rsECsz9OKVTfddBMQa6tU761Vfh5//HEg1pTXM91tFHUw0HGvsi9ikeq5//jHPwIw11xzAfFYUTl2fVVlJxbtc0qSdXWqL6wMV5mYQkxWuzKDvVjPqdp5pZD6Wz1KU05Yi4mpjgkaDykm0pX/LiapxbFNmY556tUt6jWtzjKqI60lURUdV7RyoKjLRE85WTUzMzOzZPlk1czMzMySVdfWVV999VW7tz/55JMAPPXUUzU/hyaXqDi++Jw9maTTW3Tpev/99+/2z/73v/8FYnGyonbR5Y7hw4cDcanWykk4qXvuueeAtktp6rVUXn7R/7MmFGkZ3lovNTS7XXbZBYCbb74ZiBOvVNyeWqNzNYzWxEE16FdpUTWa1KkJWjoeVJYF6ZK4WhRdcMEF7T6WFiYoLjeqbSuDyiP0urTIiS7ld3ac0ySh++67r/W2bbfdFohjq8ucGp++OLFK/7da4KFy3PTvvffeG4itA/sClbbo73o8lloQ6RK6fuekXAagS/jFNlS6Xe+BYukDwE477dTuY6b8ekWX/5dbbrnc7U888QQQW3SpfK6e9H7SBD256KKLanpcJ6tmZmZmlqyGLrcqKuCvx6QfTTRKcTnVzowbNw6IE4XUZklLx2o51hdeeKHNz06ePBmIyeqAAQMAOPHEEwHYbLPNco+tiUZqF9YM1LBfiyKozYYSMd0OsMYaawBtU9gijYcWWKhsXZMKJV26SjBt2jSgZ23ItO8oOdASeHvssQeQ3tKh2m+VBGhy0wcffNCtx3n77beB/EIjo0ePBuKVnWqUVmp5QCVuv//977u1DfWkxGPllVfO3a4JMsVkdcEFFwTie0Yp2NChQ9s8tvYrva/6UqJYpEUyNPG0kiaVNONk1DIofSxOTkpZMTEttu7qjmrLpKdIV2R0vqBJuVp8RROs6kEtN1dffXUADjnkkNz39ftMx+OecrJqZmZmZsnqlWRV9ZX6WylhV6hGVYnqbbfd1u799MlBS72mSEuk3nHHHXV7TH1CVEIlatGhZda6M+ZlWXbZZYH4Se26664D4j6wwgortN63s0RVVCf87LPPAmm1rFLbMdWVqr5YCfLYsWOBWIer8VASu8oqq7Q+llp4qS5x0KBBQPxUq9q91OjT9umnnw7E92+xebXGZNKkSQD8+c9/BmICq4buPWk7pPYtctdddwHx/VoG1ZpqXJT+TZgwAYgLrmyxxRZA24UjpDJBKV6x0Hvhl7/8JQAjRowA4j7TzDbaaCMgtlmSyt8fqmOsvGJjXaerVPWoi2209mpS+yL9nyhR1fFR8z5qSVS1LLUee9iwYUBckEfHJB1fdKVQi7F01FavK5ysmpmZmVmyeiVZVX2paoQuvfTSdu+39dZbt/5byVBxNmw1mv08ZcqUmra1WWimnWbeFX344YdAc6Qk+sSl+tKpU6cCMWk95ZRTgI7TVCWL+lSndFappBKolGi2qWZ+628tB1qcTakZnB1RPaKWNFatXupLBJ500klATEAqF4CAWJP6wAMP1O05VQ+qWlVRV4oym8MfdthhQOxuoHp2LV2oGmSl60ox1DVE9baVx1pdXVGCqs4Rql1Vklzt+NwM9tprLyDWZivt0dK5lY3Km6lTSopSP6bUaq211sp93QyvVx0adNzQAiBaNKSrFl54YSDfuUjHD82D0PwCzb0RXeUeOXIkEOfP1MrJqpmZmZklq1eSVdlzzz2B6suybrnllq3/1uzWar1biypnAfdlStu0vGYxfZMzzjgDqM/yaY2m5FR1t0OGDAFifc1f/vKXqj+7/vrrA/H1ata/alRVu1ZrvUyZlJaq/6dqKvUeqbxNdb277rorEJdbrefsz0bS/tobSxoqdVOP15Qce+yxQNy/N954YyDuA+oGoE4H6vrRlfpBpY9KqVVTtu+++wLNmazqfa6Z6vq/1ZUVHRe6kqbqyobGWolzf7lqV416lIp6+fZVxdfbDMusKsXUeZOO+5rnMHDgQCB2IFIvadWjHn300UCsR11ggQW6/Ny6mqurQldccUUPX0X7nKyamZmZWbJCRyuhhBC6tByUzsq1EopSv5lnnrnHG6aai2rJqtImJRDnn38+0P0ZvFmWha7et6vjIZqBvfTSSwPw9NNPd+fHc9Q3Uf0oi/0X5a9//SsQV3bqbs1qI8ejM5oZPnjwYD2+tin3dXu3qc7wyiuvBGJaW6tGjofqCqvV9CgVveeee4CYpP773/8GYnpa+T3VdmrWuJ7jxhtv7M6mVdWd8YD67yONoPFcaaWVADjnnHOA2FGjM43YR8aPHw/Ad77zndztF198MRBTHs0DqIVW5NExZqmllgLiPtRdvXkM0UxkdU1QcqRESfV1+j/uyFFHHQXE7hz6/aWrdsccc0yPtrHMY2o9KbVXFwXVKnZ3RadmGY/i6638/VNP9RwPpf+zzz577vZHHnkEiP3J9XtS91fdvvqwdofO0VRHf9VVV3X7MSpVGw8nq2ZmZmaWrLokq0Wqe1Dq2ZOEtVqyqpo2PbbqrXqqkZ/yjj/+eCDWcFx++eVA2xUeitQVQeukQ0wQNCu4SCsXDR8+HOj5p5syP/Xqk7rqZVRDp9S0km5TPas6BdS7Z2Ijx0PvC83K1sxNpUOdqUzN9b5QSquZmPWe0d6XklXVziud1BUi1Terd2lnq341Yh/RlRFtk5JUzfavp1tvvRWIV4BWW201INagdVcj3zPq8Xj22WcD8RhRre5YidJpp50GwFtvvdX6PfX9Vs9ezYDWc7zyyisAHHDAAQDceeed3dnUVmUeU1V3qZr+auvdd2TttdcG4mx4/b3OOuv0aJuaJVl98MEHAZg4cSLQs7HrinqOx5prrgnEenTty/Wg3zd6X+jc66OPPgLqV+vuZNXMzMzMmk5DklU56KCDgFiD2J112aslq+o1WWuiKo34lKdP+ffeey8QExyNtRKL+++/H8gnqAADBgwAurbqkGbcKWnubv1QUbN86u0tvTke6hWrrhjaL1RnVOyKUJk433333bU8dZf1pWRVnRNUE6xZ4DJ06FCg8xn2zfaeUdqiqzC6EqbV8HqamEkjx0O/F1S7r368qr3TayvW7FU8X+V25r6nqzK6KqUa1Vo7aaSwf6grgv6P1TVBX7dHiar2f6W0O++8c+727kphPLpC+8c111wDNEeyKqq313u82vtBdAWu+J5QjTzEfaXes/yLnKyamZmZWdNpaLIq6tW1+OKLA3EmWrGPWSXNnFe9kGiNbPUHq1UjP+Wpt5mSG9WC1UK1iHpM1cF2tR9tZ5rlU29v8Xjk9aVkVXTlR6vkaeb4qFGjgLZpQ1Gz7CNakefEE08E4uqAL774IhAT1htuuKGm5+mN8VAqrqsRuqK03HLLAXEGv+pwK56v9d+PPfYYAOPGjQNiFwj1rq2XFPYPpYLqDqLfvZVX4oopq5JVOfzww4GeJ6qSwnh0RTMnq6IaVr3Xq9FVO3WgKZOTVTMzMzNrOr2SrKasNz7lqQZV9SOdrfGulUEuueSS1ttefvlloP5JalGzfOrtLR6PvL6YrNaqWfaRE044AYgJkRIjzYafNGlSXZ6nWcajt6Q0HkpUNX9EPUQhrisvmvWvGtVa50NISuPRkWKdbzMmq83IyaqZmZmZNR0nq/5Uk+PxyPN45DlZbcv7SJ7HI8/jkdcs46HetKJ+6fXWLOPRW5ysmpmZmVnTcbLqTzU5Ho88j0eek9W2vI/keTzyPB55Ho88j0eek1UzMzMzazo+WTUzMzOzZPlk1czMzMyS1WHNqpmZmZlZmZysmpmZmVmyfLJqZmZmZsnyyaqZmZmZJcsnq2ZmZmaWLJ+smpmZmVmyfLJqZmZmZsn6f3lIPIKtEnjEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prng = np.random.RandomState(seed=23355553)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io \n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_augmented=[]\n",
    "for i in train_images:\n",
    "    train_images_augmented.append(i.tolist())\n",
    "\n",
    "train_labels_augmented=[]\n",
    "for i in train_labels:\n",
    "    train_labels_augmented.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "plus = cv2.imread('plus.png')\n",
    "minus = cv2.imread('moins.png')\n",
    "divide = cv2.imread('divide.png')\n",
    "multiply = cv2.imread('multiply.png')\n",
    "equal = cv2.imread('egal.png')\n",
    "operators=[plus,minus,divide,multiply,equal]\n",
    "op=[10,11,12,13,14]\n",
    "dilat=[np.ones((1,1),np.uint8),np.ones((2,2),np.uint8),np.ones((3,3),np.uint8)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the pngs of the operators to the right size and shift them to better train our cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in operators:\n",
    "    image = cv2.resize(i, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    rows,cols=image.shape[:2]\n",
    "    invert = cv2.bitwise_not(image)\n",
    "    graym = cv2.cvtColor(invert, cv2.COLOR_BGR2GRAY)\n",
    "    train_images_augmented.append(graym.tolist())\n",
    "    for v in dilat:\n",
    "        image = cv2.resize(i, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.dilate(image, v)\n",
    "        invert = cv2.bitwise_not(image)\n",
    "        gray = cv2.cvtColor(invert, cv2.COLOR_BGR2GRAY)\n",
    "        train_images_augmented.append(gray.tolist())\n",
    "        image = cv2.resize(i, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.erode(image, v)\n",
    "        invert = cv2.bitwise_not(image)\n",
    "        graym = cv2.cvtColor(invert, cv2.COLOR_BGR2GRAY)\n",
    "        train_images_augmented.append(graym.tolist())\n",
    "        for k in range(20):\n",
    "            M=np.float32([[1, 0, random.randint(-3,3)], [0, 1, random.randint(-3,3)]])\n",
    "            gray=cv2.warpAffine(gray,M,(cols,rows))\n",
    "            gray=cv2.warpAffine(graym,M,(cols,rows))\n",
    "            train_images_augmented.append(gray.tolist())\n",
    "            train_images_augmented.append(graym.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in op:\n",
    "    for j in range(1+(((20*2)+2)*len(dilat))):\n",
    "        train_labels_augmented.append(i)\n",
    "train_labels_augmented=np.array(train_labels_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_augmented=np.array(train_images_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we train our cnn here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\JambonEmmental\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60635 samples, validate on 60635 samples\n",
      "Epoch 1/10\n",
      "60635/60635 [==============================] - 126s 2ms/step - loss: 0.4726 - accuracy: 0.8551 - val_loss: 0.1042 - val_accuracy: 0.9696\n",
      "Epoch 2/10\n",
      "60635/60635 [==============================] - 132s 2ms/step - loss: 0.1106 - accuracy: 0.9669 - val_loss: 0.0605 - val_accuracy: 0.9817\n",
      "Epoch 3/10\n",
      "60635/60635 [==============================] - 138s 2ms/step - loss: 0.0792 - accuracy: 0.9757 - val_loss: 0.0426 - val_accuracy: 0.9873\n",
      "Epoch 4/10\n",
      "60635/60635 [==============================] - 131s 2ms/step - loss: 0.0591 - accuracy: 0.9819 - val_loss: 0.0328 - val_accuracy: 0.9901\n",
      "Epoch 5/10\n",
      "60635/60635 [==============================] - 133s 2ms/step - loss: 0.0513 - accuracy: 0.9840 - val_loss: 0.0317 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "60635/60635 [==============================] - 136s 2ms/step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.0230 - val_accuracy: 0.9929\n",
      "Epoch 7/10\n",
      "60635/60635 [==============================] - 145s 2ms/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.0215 - val_accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "60635/60635 [==============================] - 134s 2ms/step - loss: 0.0349 - accuracy: 0.9891 - val_loss: 0.0178 - val_accuracy: 0.9947\n",
      "Epoch 9/10\n",
      "60635/60635 [==============================] - 135s 2ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.0161 - val_accuracy: 0.9949\n",
      "Epoch 10/10\n",
      "60635/60635 [==============================] - 131s 2ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.0134 - val_accuracy: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24ea0e872c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "train_images_augmented =train_images_augmented.reshape((train_images_augmented.shape[0], 28, 28, 1)).astype('float32')\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_images_augmented= train_images_augmented / 255\n",
    "test_images = test_images / 255\n",
    "# one hot encode outputs\n",
    "train_labels_augmented= np_utils.to_categorical(train_labels_augmented)\n",
    "test_labels = np_utils.to_categorical(test_labels)\n",
    "num_classes = train_labels_augmented.shape[1]\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D())\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D())\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(train_images_augmented, train_labels_augmented, validation_data=(train_images_augmented, train_labels_augmented), epochs=10, batch_size=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load it into a pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "filename = 'digits_cnn2.joblib.pkl'\n",
    "_ = joblib.dump(model, filename, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
